1. In recurrent neural networks, with increasing the length of the input there was performance degradation. Attention mechanism fixes this.
2. Naradaya-Watson proposed a better approach in which the estimator uses a weighted average where weights correspond to relevance of the training instance to the query: ğ‘¦Ë† = âˆ‘ï¸ğ‘› ğ‘–=1 ğ›¼ (ğ‘¥, ğ‘¥ğ‘–)ğ‘¦ğ‘– . Here weighting function ğ›¼ (ğ‘¥, ğ‘¥ğ‘–) encodes the relevance of instance ğ‘¥ğ‘– to predict for ğ‘¥.
3. Challenged of traditional encoder-decoder:
   1. First, the encoder has to compress all the input information into a single fixed length vector â„\_t  that is passed to the decoder.
   2. Second, it is unable to model alignment between input and output sequences, which is an essential aspect of structured output tasks such as translation or summarization
4. The central idea is to induce attention weights ğ›¼ over the input sequence to prioritize the set of positions where relevant information is present for generating the next output token
5. Distinctive attention is used when the key and query belong to two distinct input and output sequences
6. 